<html>
	<head>
			<style type="text/css">
			body{
				margin: 3%;
			}
		</style>
		<title></title>
	</head>
	<body>
		<h1>
			<strong>Document Summarization using Clustering and Submodular Functions</strong></h1>
		<p>
			&nbsp;</p>
		<h2>
			What do we do ?</h2>
		<ul>
			<li>
				We design novel methods to do the task of Text Summarization using a class of sub-modular functions and clustering techniques.</li>
			<li>
				The sub-modular functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity.</li>
			<li>
				Our clustering techniques cluster similar types of sentences optimally.</li>
		</ul>
		<p>
			&nbsp;</p>
		<h2>
			<strong>How did we generate the summary from a collection of documents?</strong></h2>
		<p>
			Extractive text summarization process can be divided into two steps:</p>
		<ol>
			<li>
				Pre Processing step and</li>
			<li>
				Processing step.</li>
		</ol>
		<h3>
			Pre-Processing</h3>
		<p>
			In Pre-Processing is structured representation of the original text. It usually includes:</p>
		<ul>
			<li>
				Sentences boundary identification. In English, sentence boundary is identified with presence of dot at the end of sentence.</li>
			<li>
				Stop-Word Elimination. Common words with no semantics and which do not aggregate relevant information to the task are eliminated.</li>
			<li>
				Stemming - The purpose of stemming is to obtain the stem or radix of each word, which emphasize its semantics.</li>
		</ul>
		<h3>
			Processing</h3>
		<p>
			In Processing step, the coverage and diversity measures are calculated for each sentence using their tf-idf scores. Then a weight-age is appropriately given to these parameters and the final score for a sentence is calculated. The top ranking sentences are selected for the final summary.</p>
		<p>
			&nbsp;</p>
		<h2>
			How do we evaluate the generated Summary?</h2>
		<p>
			<strong>ROUGE</strong>, or Recall-Oriented Understudy for Gisting Evaluation,</p>
		<ul>
			<li>
				It is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing.</li>
			<li>
				The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.</li>
			<li>
				Rouge generates three scores (recall, precision and F-measure) for each evaluation.</li>
			<li>
				Precision and F-measure scores are useful when the target summary length is not enforced. ROUGE uses model average to compute the overall ROUGE scores when there are multiple references. The model average option is specified using A (for Average) and the best model option is specified using B (for the Best).</li>
			<li>
				There is a specific format for the system generated file as:
				<ol>
					<li type="a">
						Each of the system generated sentences should be in single line.</li>
					<li type="a">
						Output from the system should be preferably in form of plaint/text file.</li>
				</ol>
			</li>
		</ul>
		<p>
			&nbsp;</p>
		<h2>
			Dataset</h2>
		<p>
			We used the <strong>DUC-2004</strong> dataset which contains 50 TDT topics/events/timespan and a subset of the documents TDT annotators found for each topic/event/timespan. The documents were taken from the AP newswire and the New York Times newswire and subsets were formed with an average of 10 documents per subset. The dataset also contains very short summaries of each document ( 75 bytes) and a short summary<br />
			( 665 bytes) of each cluster.</p>
		<p>
			&nbsp;</p>
		<h2>
			<strong>Submodular functions</strong></h2>
		<p>
			<strong>Submodular functions</strong> share many properties in common with convex functions, one of which is that they are closed under a number of common combination operations (summation, certain compositions, restrictions, and so on). These operations give us the tools necessary to design a powerful submodular objective for submodular document summarization that extends beyond any previous work.<br />
			<br />
			We are given a set of objects<em> V = {v1, ..., vn}</em> and a function <em>F : 2V &rarr; R</em> that returns a real value for any subset <em>S &sube; V</em>. We are interested in finding the subset of bounded size |S| &le; k that maximizes the function, e.g, argmax S&sube;V F(S). <strong>Sub-modular functions are those that satisfy the property of <em>diminishing returns:</em></strong> for any A &sube; B &sube; V v, a sub-modular function F must satisfy <strong><em>F(A + v) &minus; F(A) &ge; F(B + v) &minus; F(B).</em></strong> That is, the incremental value of v decreases as the context in which v is considered grows from A to B. F is monotone non-decreasing if &forall;A &sube; B, F(A) &ge; F(B)</p>
		<p>
			&nbsp;</p>
		<h2>
			<strong>Properties of a good Summary</strong></h2>
		<p>
			Two properties of a good summary are</p>
		<ol>
			<li>
				Relevance and</li>
			<li>
				Diversity i.e., non- redundancy.</li>
		</ol>
		<p>
			Objective functions for extractive summarization usually measure these two separately and then mix them together trading off encouraging relevance and penalizing redundancy. <em>The redundancy penalty usually violates the monotonicity of the objective functions</em>. <em>We therefore propose to positively reward diversity instead of negatively penalizing redundancy.</em> In particular, we model the summary quality as</p>
		<p style="margin-left: 240px;">
			<em>F(S) = L(S) + &lambda;R(S)</em></p>
		<p>
			<em>L(S) measures the coverage, or fidelity, of summary set S to the document, </em><br />
			<em>R(S) rewards diversity in S, and </em><br />
			<em><em>&lambda; </em>is a trade-off coefficient</em></p>
		<h3>
			Coverage Measure,L(S):</h3>
		<p>
			L(S) can be interpreted either as a set function that measures the similarity of summary set S to the document to be summarized, or as a function representing some form of coverage of V by S.<br />
			Most naturally, <em><strong>L(S) should be monotone</strong>, </em>as coverage improves with a larger summary.<br />
			<strong>L(S) should also be submodular:</strong> consider adding a new sentence into two summary sets, one a subset of the other. Intuitively, the increment when adding a new sentence to the small summary set should be larger than the increment when adding it to the larger set, as the information carried by the new sentence might have already been covered by those sentences that are in the larger summary but not in the smaller<br />
			summary. This is exactly the property of diminishing returns.</p>
		<h3>
			Diversity Measure,R(S):</h3>
		<p>
			Instead of penalizing redundancy by subtracting from the objective, we propose to reward diversity. The function R(S) rewards diversity in that there is usually more benefit to selecting a sentence from a cluster not yet having one of its elements already chosen. As soon as an element is selected from a cluster, other elements from the same cluster start having diminishing gain, thanks to the square root function.</p>
		<p>
			&nbsp;</p>
		<p>
			&nbsp;</p>
	</body>
</html>
