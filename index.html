<html>
	<head>
			<style type="text/css">
			body{
				margin: 3%;
			}
		</style>
		<title></title>
	</head>
	<body>
		<h1>
			<strong>Document Summarization using Clustering and Submodular Functions</strong></h1>
		<p>
			&nbsp;</p>
		<h2>
			What do we do ?</h2>
		<ul>
			<li>
				We design novel methods to do the task of Text Summarization using a class of sub-modular functions and clustering techniques.</li>
			<li>
				The sub-modular functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity.</li>
			<li>
				Our clustering techniques cluster similar types of sentences optimally.</li>
		</ul>
		<p>
			&nbsp;</p>
		<h2>
			<strong>How did we generate the summary from a collection of documents?</strong></h2>
		<p>
			Extractive text summarization process can be divided into two steps:</p>
		<ol>
			<li>
				Pre Processing step and</li>
			<li>
				Processing step.</li>
		</ol>
		<h3>
			Pre-Processing</h3>
		<p>
			In Pre-Processing is structured representation of the original text. It usually includes:</p>
		<ul>
			<li>
				Sentences boundary identification. In English, sentence boundary is identified with presence of dot at the end of sentence.</li>
			<li>
				Stop-Word Elimination. Common words with no semantics and which do not aggregate relevant information to the task are eliminated.</li>
			<li>
				Stemming - The purpose of stemming is to obtain the stem or radix of each word, which emphasize its semantics.</li>
		</ul>
		<h3>
			Processing</h3>
		<p>
			In Processing step, the coverage and diversity measures are calculated for each sentence using their tf-idf scores. Then a weight-age is appropriately given to these parameters and the final score for a sentence is calculated. The top ranking sentences are selected for the final summary.</p>
		<p>
			&nbsp;</p>
		<h2>
			How do we evaluate the generated Summary?</h2>
		<p>
			<strong>ROUGE</strong>, or Recall-Oriented Understudy for Gisting Evaluation,</p>
		<ul>
			<li>
				It is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing.</li>
			<li>
				The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.</li>
			<li>
				Rouge generates three scores (recall, precision and F-measure) for each evaluation.</li>
			<li>
				Precision and F-measure scores are useful when the target summary length is not enforced. ROUGE uses model average to compute the overall ROUGE scores when there are multiple references. The model average option is specified using A (for Average) and the best model option is specified using B (for the Best).</li>
			<li>
				There is a specific format for the system generated file as:
				<ol>
					<li type="a">
						Each of the system generated sentences should be in single line.</li>
					<li type="a">
						Output from the system should be preferably in form of plaint/text file.</li>
				</ol>
			</li>
		</ul>
		<p>
			&nbsp;</p>
		<h2>
			Dataset</h2>
		<p>
			We used the <strong>DUC-2004</strong> dataset which contains 50 TDT topics/events/timespan and a subset of the documents TDT annotators found for each topic/event/timespan. The documents were taken from the AP newswire and the New York Times newswire and subsets were formed with an average of 10 documents per subset. The dataset also contains very short summaries of each document ( 75 bytes) and a short summary<br />
			( 665 bytes) of each cluster.</p>
	</body>
</html>
